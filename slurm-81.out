2022-11-07 15:14:11,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://129.88.193.194:35397'
2022-11-07 15:14:11,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://129.88.193.194:44049'
2022-11-07 15:14:11,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://129.88.193.194:41581'
2022-11-07 15:14:11,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://129.88.193.194:46695'
2022-11-07 15:14:11,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://129.88.193.194:39035'
2022-11-07 15:14:11,931 - distributed.worker - INFO -       Start worker at: tcp://129.88.193.194:46643
2022-11-07 15:14:11,931 - distributed.worker - INFO -          Listening to: tcp://129.88.193.194:46643
2022-11-07 15:14:11,931 - distributed.worker - INFO -           Worker name:           SLURMCluster-4-2
2022-11-07 15:14:11,931 - distributed.worker - INFO -          dashboard at:       129.88.193.194:35695
2022-11-07 15:14:11,931 - distributed.worker - INFO - Waiting to connect to: tcp://129.88.193.194:34409
2022-11-07 15:14:11,931 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,931 - distributed.worker - INFO -               Threads:                          4
2022-11-07 15:14:11,932 - distributed.worker - INFO -                Memory:                  13.97 GiB
2022-11-07 15:14:11,932 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vywep25g
2022-11-07 15:14:11,932 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,939 - distributed.worker - INFO -       Start worker at: tcp://129.88.193.194:33499
2022-11-07 15:14:11,939 - distributed.worker - INFO -          Listening to: tcp://129.88.193.194:33499
2022-11-07 15:14:11,939 - distributed.worker - INFO -           Worker name:           SLURMCluster-4-3
2022-11-07 15:14:11,939 - distributed.worker - INFO -          dashboard at:       129.88.193.194:37577
2022-11-07 15:14:11,939 - distributed.worker - INFO - Waiting to connect to: tcp://129.88.193.194:34409
2022-11-07 15:14:11,939 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,939 - distributed.worker - INFO -               Threads:                          4
2022-11-07 15:14:11,940 - distributed.worker - INFO -                Memory:                  13.97 GiB
2022-11-07 15:14:11,940 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c65gz6jh
2022-11-07 15:14:11,940 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,940 - distributed.worker - INFO -       Start worker at: tcp://129.88.193.194:46817
2022-11-07 15:14:11,940 - distributed.worker - INFO -          Listening to: tcp://129.88.193.194:46817
2022-11-07 15:14:11,940 - distributed.worker - INFO -           Worker name:           SLURMCluster-4-1
2022-11-07 15:14:11,940 - distributed.worker - INFO -          dashboard at:       129.88.193.194:43335
2022-11-07 15:14:11,940 - distributed.worker - INFO - Waiting to connect to: tcp://129.88.193.194:34409
2022-11-07 15:14:11,940 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,940 - distributed.worker - INFO -               Threads:                          4
2022-11-07 15:14:11,940 - distributed.worker - INFO -                Memory:                  13.97 GiB
2022-11-07 15:14:11,940 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dbhmjhtf
2022-11-07 15:14:11,940 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,946 - distributed.worker - INFO -         Registered to: tcp://129.88.193.194:34409
2022-11-07 15:14:11,946 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,946 - distributed.core - INFO - Starting established connection
2022-11-07 15:14:11,953 - distributed.worker - INFO -         Registered to: tcp://129.88.193.194:34409
2022-11-07 15:14:11,953 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,953 - distributed.core - INFO - Starting established connection
2022-11-07 15:14:11,954 - distributed.worker - INFO -         Registered to: tcp://129.88.193.194:34409
2022-11-07 15:14:11,954 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,955 - distributed.core - INFO - Starting established connection
2022-11-07 15:14:11,982 - distributed.worker - INFO -       Start worker at: tcp://129.88.193.194:36927
2022-11-07 15:14:11,982 - distributed.worker - INFO -          Listening to: tcp://129.88.193.194:36927
2022-11-07 15:14:11,982 - distributed.worker - INFO -           Worker name:           SLURMCluster-4-0
2022-11-07 15:14:11,982 - distributed.worker - INFO -          dashboard at:       129.88.193.194:39063
2022-11-07 15:14:11,982 - distributed.worker - INFO - Waiting to connect to: tcp://129.88.193.194:34409
2022-11-07 15:14:11,982 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,982 - distributed.worker - INFO -               Threads:                          4
2022-11-07 15:14:11,982 - distributed.worker - INFO -                Memory:                  13.97 GiB
2022-11-07 15:14:11,982 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-llzjxspp
2022-11-07 15:14:11,982 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,993 - distributed.worker - INFO -         Registered to: tcp://129.88.193.194:34409
2022-11-07 15:14:11,994 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:11,994 - distributed.core - INFO - Starting established connection
2022-11-07 15:14:12,001 - distributed.worker - INFO -       Start worker at: tcp://129.88.193.194:44697
2022-11-07 15:14:12,001 - distributed.worker - INFO -          Listening to: tcp://129.88.193.194:44697
2022-11-07 15:14:12,001 - distributed.worker - INFO -           Worker name:           SLURMCluster-4-4
2022-11-07 15:14:12,001 - distributed.worker - INFO -          dashboard at:       129.88.193.194:44171
2022-11-07 15:14:12,001 - distributed.worker - INFO - Waiting to connect to: tcp://129.88.193.194:34409
2022-11-07 15:14:12,001 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:12,001 - distributed.worker - INFO -               Threads:                          4
2022-11-07 15:14:12,001 - distributed.worker - INFO -                Memory:                  13.97 GiB
2022-11-07 15:14:12,001 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fkv77p9g
2022-11-07 15:14:12,001 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:12,013 - distributed.worker - INFO -         Registered to: tcp://129.88.193.194:34409
2022-11-07 15:14:12,013 - distributed.worker - INFO - -------------------------------------------------
2022-11-07 15:14:12,013 - distributed.core - INFO - Starting established connection
/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered
  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,
/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered
  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,
/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered
  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,
/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered
  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,
/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered
  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,
slurmstepd-protect-slr: error: *** JOB 81 ON protect-slr CANCELLED AT 2022-11-07T15:15:29 ***
2022-11-07 15:15:29,903 - distributed.worker - INFO - Stopping worker at tcp://129.88.193.194:44697
2022-11-07 15:15:29,897 - distributed.worker - INFO - Stopping worker at tcp://129.88.193.194:33499
2022-11-07 15:15:29,905 - distributed.worker - INFO - Stopping worker at tcp://129.88.193.194:46817
2022-11-07 15:15:29,904 - distributed.worker - INFO - Stopping worker at tcp://129.88.193.194:46643
2022-11-07 15:15:29,924 - distributed.worker - INFO - Stopping worker at tcp://129.88.193.194:36927
2022-11-07 15:15:30,073 - distributed.worker - INFO - Connection to scheduler broken. Closing without reporting. ID: Worker-9f6dfafd-9f42-430a-8153-1d7609a0983c Address tcp://129.88.193.194:36927 Status: Status.closing
2022-11-07 15:15:30,073 - distributed.worker - INFO - Connection to scheduler broken. Closing without reporting. ID: Worker-0e700dc6-3131-4c08-8383-eea8b6b48609 Address tcp://129.88.193.194:44697 Status: Status.closing
2022-11-07 15:15:30,081 - distributed.worker - INFO - Connection to scheduler broken. Closing without reporting. ID: Worker-2f101c60-a5b2-4a9e-90fc-a2123b1eab08 Address tcp://129.88.193.194:46643 Status: Status.closing
2022-11-07 15:15:30,082 - distributed.worker - INFO - Connection to scheduler broken. Closing without reporting. ID: Worker-61f31fe7-b4e0-4eee-98b7-da13483dfb86 Address tcp://129.88.193.194:46817 Status: Status.closing
2022-11-07 15:15:30,100 - distributed.worker - INFO - Connection to scheduler broken. Closing without reporting. ID: Worker-483d20e1-113f-4430-9f69-5bfe7e75bea3 Address tcp://129.88.193.194:33499 Status: Status.closing
2022-11-07 15:15:30,105 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://129.88.193.194:39964 remote=tcp://129.88.193.194:34409>
Traceback (most recent call last):
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/tornado/gen.py", line 762, in run
    value = future.result()
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2022-11-07 15:15:30,099 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://129.88.193.194:39960 remote=tcp://129.88.193.194:34409>
Traceback (most recent call last):
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/tornado/gen.py", line 762, in run
    value = future.result()
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2022-11-07 15:15:30,102 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://129.88.193.194:39968 remote=tcp://129.88.193.194:34409>
Traceback (most recent call last):
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/tornado/gen.py", line 762, in run
    value = future.result()
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2022-11-07 15:15:30,101 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://129.88.193.194:39966 remote=tcp://129.88.193.194:34409>
Traceback (most recent call last):
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/tornado/gen.py", line 762, in run
    value = future.result()
  File "/home/barreje/miniconda3/envs/earthgif/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
